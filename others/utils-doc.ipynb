{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%autoreload 2 \n",
    "import utils\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 80), (2, 170), (3, 100), (3, 220), (4, 200), (4, 270), (5, 500)]\n",
    "def f(x):\n",
    "    return 60*x #number of bedrooms * 60k$ = price of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7628.571428571428"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.mse_non_vectorized(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7628.571428571428"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [f(x) for (x, y) in data]\n",
    "y = [y for (x, y) in data]\n",
    "utils.mse_vectorized(np.array(y), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{x}_b^T\\mathbf{x}_b)^{-1}\\mathbf{x}_b^T \\mathbf{y}$$\n",
    "\n",
    "Implementations:\n",
    "```python\n",
    "def normal_equation_linear_regression(x,y):\n",
    "    intercept_ones = np.ones((len(x),1)) # results in array( [ [1],..,[1] ] )\n",
    "    x_b = np.c_[intercept_ones,x] # we now add the additional ones as a new column to our X\n",
    "    theta_optimal = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y) # the normal equation\n",
    "    return theta_optimal\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46.31578947,  84.73684211])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array([\n",
    "    (1,80),\n",
    "    (2,170),\n",
    "    (3,100),\n",
    "    (3,220),\n",
    "    (4,200),\n",
    "    (4,270),\n",
    "    (5,500)\n",
    "    ])\n",
    "\n",
    "x = dataset[:,0]\n",
    "y = dataset[:,1]\n",
    "\n",
    "utils.normal_equation_linear_regression(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-46.315789473684276 [84.73684211]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x = np.array([[1], [2], [3], [3], [4], [4], [5]])\n",
    "y = np.array([ 80, 170, 100, 220, 200, 270, 500])\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x,y)\n",
    "print(linreg.intercept_, linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_predict = linreg.predict(x)\n",
    "\n",
    "plt.plot(x, y_predict, \"g-\") #g- means: use green as the color (\"g\"), and draw a line (\"-\")\n",
    "plt.plot(x, y, \"b.\") #b. means: use blue as the color (\"b\"), and draw individual points (\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Linear Regression\n",
    "\n",
    "$$y = mx + b$$\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial m} = \\frac{-2}{n} \\sum_{i=1}^n{((y_i-{\\color{#26a6ed}(mx+b)}) \\cdot x_i)} \\\\\\\\\n",
    "\n",
    "\\frac{\\partial MSE}{\\partial b} = \\frac{-2}{n} \\sum_{i=1}^n{(y_i-{\\color{#26a6ed}(mx+b)})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla(MSE) = \\left[\\begin{array}{c} \n",
    "\n",
    "    \\frac{-2}{n} \\sum_{i=1}^n{((y_i-{\\color{#26a6ed}(mx+b)}) \\cdot x_i)} \\\\\\\\\n",
    "\n",
    "    \\frac{-2}{n} \\sum_{i=1}^n{(y_i-{\\color{#26a6ed}(mx+b)})}\n",
    "\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([\n",
    "    (1,80),\n",
    "    (2,170),\n",
    "    (3,100),\n",
    "    (3,220),\n",
    "    (4,200),\n",
    "    (4,270),\n",
    "    (5,500)])\n",
    "\n",
    "x = dataset[:,0]\n",
    "y = dataset[:,1]\n",
    "\n",
    "def add_intercept_ones(X):\n",
    "    intercept_ones = np.ones((len(X),1)) # results in array( [ [1],..,[1] ] )\n",
    "    X_b = np.c_[intercept_ones,X]\n",
    "    return X_b\n",
    "\n",
    "X_b = add_intercept_ones(x)\n",
    "\n",
    "# Making Predictions\n",
    "def create_function(theta):\n",
    "    def f(X_b):\n",
    "        return np.dot(X_b,theta)\n",
    "    return f\n",
    "\n",
    "# Calculating derivatives\n",
    "\n",
    "def mse_derivative_m(X,y,y_predicted):\n",
    "    return -(2/y.size) * sum(X * (y - y_predicted))\n",
    "    \n",
    "def mse_derivative_b(y,y_predicted):\n",
    "    return -(2/y.size) * sum(y - y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Loss \n",
    "$$MSE(m,b) = \\frac{1}{n} \\cdot ( (y_i-{\\color{#26a6ed}y_{pred}})^T \\cdot (y_i-{\\color{#26a6ed}y_{pred}}) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_predicted):\n",
    "    error = y-y_predicted\n",
    "    loss = 1/(y.size) * np.dot(error.T, error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, criterion, number_of_iterations, learning_rate):\n",
    "    X_b = add_intercept_ones(x)\n",
    "    loss_history=[]\n",
    "    for i in range(number_of_iterations):\n",
    "        \n",
    "        # predict and calculate loss\n",
    "        f = create_function(theta) # create the current function\n",
    "        y_predicted = f(X_b) # predict our entire x\n",
    "        loss = criterion(y,y_predicted) # calculate the error\n",
    "        loss_history.append(loss)\n",
    "            \n",
    "        # perform optimization\n",
    "        gradient = np.array([mse_derivative_b(y,y_predicted), mse_derivative_m(x,y,y_predicted)]) # calculate gradient\n",
    "        theta = theta - learning_rate * gradient #adjust m and b\n",
    "\n",
    "        if i%10==0:\n",
    "            print(\"Current Epoch: {}, Current Loss: {}\".format(i,loss))\n",
    "        \n",
    "    return theta,loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 0, Current Loss: 65228.57142857143\n",
      "Current Epoch: 10, Current Loss: 7238.891593802805\n",
      "Current Epoch: 20, Current Loss: 6238.347951740184\n",
      "Current Epoch: 30, Current Loss: 6202.096015302292\n",
      "Current Epoch: 40, Current Loss: 6182.86618019306\n",
      "Current Epoch: 50, Current Loss: 6164.615769307687\n",
      "Current Epoch: 60, Current Loss: 6147.047846616225\n",
      "Current Epoch: 70, Current Loss: 6130.132652256769\n",
      "Current Epoch: 80, Current Loss: 6113.845862732123\n",
      "Current Epoch: 90, Current Loss: 6098.164131391666\n",
      "Current Epoch: 100, Current Loss: 6083.064980156502\n",
      "Current Epoch: 110, Current Loss: 6068.526766034719\n",
      "Current Epoch: 120, Current Loss: 6054.528650077142\n",
      "Current Epoch: 130, Current Loss: 6041.050567506585\n",
      "Current Epoch: 140, Current Loss: 6028.073198957181\n",
      "Current Epoch: 150, Current Loss: 6015.577942782173\n",
      "Current Epoch: 160, Current Loss: 6003.546888390466\n",
      "Current Epoch: 170, Current Loss: 5991.962790573754\n",
      "Current Epoch: 180, Current Loss: 5980.8090447873765\n",
      "Current Epoch: 190, Current Loss: 5970.069663349519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.89537946, 72.3849704 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([0.,0.])\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0075\n",
    "ideal_theta, loss_history = gradient_descent(x, y, theta, mse, num_epochs, learning_rate)\n",
    "ideal_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14be488b0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(num_epochs),loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68.95520002] [12.285233]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "X = np.array([[1], [2], [3], [3], [4], [4], [5]])\n",
    "sgdreg = SGDRegressor(learning_rate=\"constant\",eta0=0.0075,max_iter=200,random_state=42) #eta0 is our initial learning rate\n",
    "sgdreg.fit(X,y)\n",
    "print(sgdreg.coef_,sgdreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Split Your Dataset the Right Way\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42, shuffle=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "$$Ridge\\ Loss(y,y_{pred}) = MSE(y,y_{pred}) + ||\\boldsymbol{\\theta}||_2^2 \\\\$$\n",
    "$$Ridge\\ Loss(y,y_{pred}) = MSE(y,y_{pred}) + \\sum_{i=1}^m{\\theta_i^2} \\\\$$\n",
    "$$Ridge\\ Loss(y,y_{pred}) = MSE(y,y_{pred}) + \\boldsymbol{\\theta}^T\\theta \\\\$$\n",
    "\n",
    "## Solving Ridge Regression\n",
    "\n",
    "### Normal Equation\n",
    "$$\\begin{aligned}\n",
    "\n",
    "Ridge\\ Loss(\\textbf{y},\\textbf{y}_{pred}) &= {\\color{#26a6ed}MSE(\\textbf{y},\\textbf{y}_{pred})} + \\alpha \\cdot ||\\boldsymbol{\\theta}||_2^2 \\\\\n",
    "\n",
    "&= {\\color{#26a6ed}(\\textbf{y} − \\textbf{X}\\boldsymbol{\\theta})^T (\\textbf{y} − \\textbf{X}\\boldsymbol{\\theta})} + \\alpha\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (\\textbf{X}^T\\textbf{X} + \\alpha\\textbf{I})^{-1} \\textbf{X}^T \\textbf{y} \\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation_ridge_regression(X,y,alpha):\n",
    "    intercept_ones = np.ones((len(X),1))        # results in array( [ [1],..,[1] ] )\n",
    "    X_b = np.c_[intercept_ones,X]               # we now add the additional ones as a new column to our X\n",
    "    I = np.identity(X_b.shape[1])               # identity matrix with dimensions (n+1)\n",
    "    I[0][0] = 0                                 # adjusting the first value in I to be 0, to account for the intercept term\n",
    "    theta_optimal = np.linalg.inv(X_b.T.dot(X_b) + alpha * I).dot(X_b.T).dot(y) # the normal equation\n",
    "    return theta_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using normal equation\n",
    "\n",
    "```python\n",
    "theta_ridge_ne_1 = normal_equation_ridge_regression(X_train, y_train, 1)\n",
    "print(theta_ridge_ne_1)\n",
    "# output: [44.35714286 -2.76190476]\n",
    "```\n",
    "\n",
    "Using scikit-learns Ridge-class\n",
    "```python\n",
    "ridge = Ridge()  # same as Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train, 1)\n",
    "theta_ridge_ne_2 = np.array([ridge.intercept_, ridge.coef_[0]])\n",
    "print(theta_ridge_ne_2)\n",
    "# output: [44.35714286 -2.76190476]\n",
    "```\n",
    "\n",
    "Ploting data \n",
    "\n",
    "```python\n",
    "X_interval = np.array([min(X), max(X)]) # we'll use these points to plot our linear functions\n",
    "\n",
    "plt.plot(X_train, y_train, \"s\", markersize=12, label=\"train data\")\n",
    "plt.plot(X_test, y_test, \".\", markersize=15, label=\"test data\")\n",
    "\n",
    "plt.plot(X_interval, ols.predict(X_interval), label=\"ols\")\n",
    "plt.plot(X_interval, ridge.predict(X_interval) ,label=\"ridge\")\n",
    "plt.legend()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent\n",
    "\n",
    "$$\\begin{aligned}\n",
    "RidgeMSE(y,y_{pred}) &= {\\color{#26a6ed}MSE(y,{\\color{9628d9}y_{pred}})} + {\\color{#54C667}\\alpha \\cdot \\boldsymbol{\\theta}^T\\boldsymbol{\\theta}} \\\\\n",
    "&=  {\\color{#26a6ed}\\frac{1}{n} \\cdot  ( (y-{\\color{9628d9}y_{pred}})^T (y-{\\color{9628d9}y_{pred}}) )} + {\\color{#54C667}\\alpha \\cdot \\boldsymbol{\\theta}^T\\boldsymbol{\\theta}} \\\\\n",
    "&=  {\\color{#26a6ed}\\frac{1}{n} \\cdot  ( (y- {\\color{9628d9}\\mathbf{X}_b \\boldsymbol{\\theta}} )^T (y- {\\color{9628d9}\\mathbf{X}_b \\boldsymbol{\\theta}}) )} + {\\color{#54C667}\\alpha \\cdot \\boldsymbol{\\theta}^T\\boldsymbol{\\theta}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla(RidgeMSE) &=  {\\color{#26a6ed}\\frac{-2}{n} \\mathbf{X}_b^T (y - {\\color{9628d9}y_{pred}} )} + {\\color{#54C667}2 \\cdot \\alpha \\cdot \\boldsymbol{\\theta}} \\\\\n",
    "&= {\\color{#26a6ed}\\frac{-2}{n} \\mathbf{X}_b^T (y -  {\\color{9628d9}\\mathbf{X}_b \\boldsymbol{\\theta}})} + {\\color{#54C667}2 \\cdot \\alpha \\cdot \\boldsymbol{\\theta}}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRidgeGradientFunction(alpha=0.0001):\n",
    "    \n",
    "    def ridgeGradient(X_b, y, y_pred, theta):\n",
    "        return  -(2/y.size) * X_b.T.dot((y - y_pred)) + alpha * theta\n",
    "\n",
    "    return ridgeGradient\n",
    "\n",
    "def getRidgeMSEFunction(alpha=0.0001):\n",
    "    \n",
    "    def ridgeMSE(y, y_predicted, theta):\n",
    "        mse_loss = mse(y, y_predicted)\n",
    "        ridge_loss = mse_loss + alpha * np.dot(theta,theta)\n",
    "        return ridge_loss\n",
    "\n",
    "    return ridgeMSE\n",
    "\n",
    "\n",
    "def gradientDescent(X, y, theta, criterion, gradientFunction, number_of_iterations, learning_rate):\n",
    "    X_b = add_intercept_ones(X)\n",
    "    for i in range(number_of_iterations):\n",
    "\n",
    "        # predict and calculate loss\n",
    "        f = create_function(theta)               # create the current function\n",
    "        y_predicted = X_b.dot(theta)             # predict our entire x\n",
    "        loss = criterion(y, y_predicted, theta)  # calculate loss\n",
    "\n",
    "        # perform optimization\n",
    "        gradient = gradientFunction(X_b, y, y_predicted, theta) # compute gradient\n",
    "        theta = theta - learning_rate * gradient                # adjust theta\n",
    "\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "theta_ridge_gd_1 = gradientDescent(X_train, y_train, theta, getRidgeMSEFunction(0.0001),\n",
    "                                            getRidgeGradientFunction(0.0001), 1000, 0.03)\n",
    "print(theta_ridge_gd_1)\n",
    "# output: [44.41801778 -3.11386069]\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.plot(X_train, y_train, \"s\", markersize=12, label=\"train data\")\n",
    "plt.plot(X_test, y_test, \".\", markersize=15, label=\"test data\")\n",
    "\n",
    "plt.plot(X, X_b.dot(theta_ridge_gd_1), label=\"ridge gd\")\n",
    "plt.plot(X, X_b.dot(theta_ridge_ne_1), label=\"ridge ne\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor-class\n",
    "\n",
    "```python\n",
    "sgdreg_ridge = SGDRegressor(penalty=\"l2\", learning_rate=\"constant\", eta0=0.001, max_iter=10000, random_state=42)\n",
    "sgdreg_ridge.fit(X_train,y_train)\n",
    "theta_ridge_gd_2 = np.array([sgdreg_ridge.intercept_[0], sgdreg_ridge.coef_[0]])\n",
    "print(theta_ridge_gd_2)\n",
    "# output: [43.1764243 -2.0467813]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Value for $\\alpha$\n",
    "\n",
    "```python\n",
    "ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10])\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "print(ridge_cv.alpha_)\n",
    "# output: 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "$$New\\ Loss\\ 1(y,y_{pred}) = MSE(y,y_{pred}) + \\alpha \\sum_{i=1}^m{{\\color{#26a6ed}\\theta_i^2}} = {\\color{#26a6ed}RidgeMSE} \\\\\n",
    "\n",
    "New\\ Loss\\ 2(y,y_{pred}) = MSE(y,y_{pred}) + \\alpha \\sum_{i=1}^m{{\\color{#26a6ed}|\\theta_i|}} = {\\color{#26a6ed}LassoMSE}\n",
    "\n",
    "$$\n",
    "\n",
    "$LassoMSE(y,y_{pred}) = MSE(y,y_{pred}) + \\alpha ||\\boldsymbol{\\theta}||_1 \\\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Lasso Regression\n",
    "\n",
    "$$\\begin{aligned}\n",
    "LassoMSE(y,y_{pred},\\boldsymbol{\\theta}) &= MSE(y,y_{pred}) + \\alpha \\cdot \\sum_{i=1}^m{|\\theta_i|} \\\\\n",
    "\n",
    "\\nabla(LassoMSE) &= \\nabla(MSE(y,y_{pred})) + \\alpha \\cdot {\\color{#FF8900}\\nabla(\\sum_{i=1}^m{|\\theta_i|})} \\\\\n",
    "\n",
    "&= \\nabla(MSE(y,y_{pred})) + \\alpha \\cdot \\sum_{i=1}^m{{\\color{#FF8900}\\nabla(|\\theta_i|)}}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Lasso using Scikit-Learn\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_s_train = scaler.transform(X_train)\n",
    "X_s_test = scaler.transform(X_test)\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_s_train, y_train)\n",
    "print(lasso.coef_[0], lasso.intercept_)\n",
    "# prints: -2.5269426990176953 40.63053220519125\n",
    "\n",
    "# OR\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), Lasso())\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline[1].coef_[0], pipeline[1].intercept_)\n",
    "# prints: -2.5269426990176953 40.63053220519125\n",
    "\n",
    "# OR: SGDRegressor\n",
    "pipeline_sgd = make_pipeline(StandardScaler(), SGDRegressor(alpha=1, penalty=\"l1\"))\n",
    "pipeline_sgd.fit(X_train, y_train)\n",
    "print(pipeline_sgd[1].coef_[0], pipeline_sgd[1].intercept_)\n",
    "# prints: -2.2233194459768386 [40.87110655]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Value for $\\alpha$\n",
    "\n",
    "```python\n",
    "lasso_cv_pipeline = make_pipeline(StandardScaler(),\n",
    "                                  LassoCV(alphas=[0.1, 1.0, 10]))\n",
    "lasso_cv_pipeline.fit(X_train, y_train)\n",
    "print(lasso_cv_pipeline[1].alpha_)\n",
    "# output: 0.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "$$\\begin{aligned}\n",
    "{\\color{#26a6ed}RidgeMSE}(y,y_{pred}) &= MSE(y,y_{pred}) + \\alpha \\sum_{i=1}^m{{\\color{#26a6ed}\\theta_i^2}} \\\\\n",
    "&= MSE(y,y_{pred}) + \\alpha {\\color{#26a6ed}||\\boldsymbol{\\theta}||_2^2} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\begin{aligned}\n",
    "{\\color{#26a6ed}LassoMSE}(y,y_{pred}) &= MSE(y,y_{pred}) + \\alpha \\sum_{i=1}^m{{\\color{#26a6ed}|\\theta_i|}} \\\\\n",
    "&= MSE(y,y_{pred}) + \\alpha {\\color{#26a6ed}||\\boldsymbol{\\theta}||_1} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\\begin{aligned}\n",
    "{\\color{#26a6ed}ElasticNetMSE} &= MSE(y,y_{pred}) + \\alpha_1 \\sum_{i=1}^m{{\\color{#26a6ed}|\\theta_i|}} +  \\alpha_2 \\sum_{i=1}^m{{\\color{#26a6ed}\\theta_i^2}} \\\\\n",
    "&= MSE(y,y_{pred}) + \\alpha_1 {\\color{#26a6ed}||\\boldsymbol{\\theta}||_1} + \\alpha_2 {\\color{#26a6ed}||\\boldsymbol{\\theta}||_2^2}  \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Elastic Net\n",
    "\n",
    "```python\n",
    "elastic_pipeline = make_pipeline(StandardScaler(),\n",
    "                                  ElasticNet(alpha=1, l1_ratio=0.1))\n",
    "elastic_pipeline.fit(X_train, y_train)\n",
    "print(elastic_pipeline[1].intercept_, elastic_pipeline[1].coef_)\n",
    "# output: 41.0 [-1.2127174]\n",
    "\n",
    "OR:\n",
    "elastic_sgd_pipeline = make_pipeline(StandardScaler(), SGDRegressor(alpha=1, l1_ratio=0.1, penalty = \"elasticnet\"))\n",
    "                         \n",
    "elastic_sgd_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(elastic_sgd_pipeline[1].intercept_, elastic_sgd_pipeline[1].coef_)\n",
    "# output: [40.69570804] [-1.21309447]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal value for $\\alpha$ and the L1-ratio\n",
    "\n",
    "```python\n",
    "elastic_cv_pipeline = make_pipeline(StandardScaler(),\n",
    "                                  ElasticNetCV(l1_ratio=0.1))\n",
    "elastic_cv_pipeline.fit(X_train, y_train)\n",
    "print(elastic_cv_pipeline[1].alpha_)\n",
    "# output: 0.6385668121344372\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Summary of Scikit-Learn-Classes\n",
    "\n",
    "| MODEL / SOLVER | NORMAL EQUATION                                                                                                                             | GRADIENT DESCENT VARIANT                                                                                                                                                                |\n",
    "| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| OLS Regression | [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression) | [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)                                                                                |\n",
    "| Ridge          | [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)                                                  | [SGDRegressor with penalty=“l2”](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)                                                              |\n",
    "| Lasso          | /                                                                                                                                           | [Lasso \\[Coordinate Descent\\] or SGDRegressor with penalty=“l1” \\[Truncated SGD\\]](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)                   |\n",
    "| Elastic Net    | /                                                                                                                                           | [ElasticNet \\[Coordinate Descent\\] or SGDRegressor with penalty=“elasticnet” \\[Truncated SGD\\]](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When, Why, And How You Should Standardize Your Data\n",
    "$$ \\large z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_mean = np.mean(X_train)\n",
    "X_train_std = np.std(X_train)\n",
    "\n",
    "X_train_standardized = (X_train - X_train_mean) / X_train_std\n",
    "X_test_standardized = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train_standardized, y_train)\n",
    "print(ridge.intercept_, ridge.coef_[0])\n",
    "# outputs: 40.7 -1.56\n",
    "\n",
    "ridge_predictions = ridge.predict(X_test_standardized)\n",
    "print(ridge_predictions)\n",
    "# outputs: [40.48 41.35 39.61]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Standardization using Scikit-Learn’s StandardScaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "Instead of writing \n",
    "\n",
    "`scaler.fit(X_train)` and `X_train_standardized = scaler.transform(X_train)` \n",
    "\n",
    "we can also just write \n",
    "\n",
    "`X_train_standardized = scaler.fit_transform(X_train)` \n",
    "\n",
    "to save us one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Standardization using Scikit-Learn’s Pipeline\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                    ('ridge', Ridge())])\n",
    "```\n",
    "\n",
    "OR:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(StandardScaler(), Ridge())  \n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline[1].intercept_, pipeline[1].coef_[0])\n",
    "# 40.7 -1.56\n",
    "pipeline_predictions = ridge.predict(X_test_standardized)\n",
    "print(pipeline_predictions)\n",
    "# [40.48 41.35 39.61]\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the gradient descent for linear regression article\n",
    "def create_function(theta):\n",
    "    def f(X_b):\n",
    "        return np.dot(X_b,theta)\n",
    "    \n",
    "    return f\n",
    "\n",
    "# from the gradient descent for linear regression article\n",
    "def add_intercept_ones(X):\n",
    "    intercept_ones = np.ones((len(X),1)) # results in array( [ [1],..,[1] ] )\n",
    "    X_b = np.c_[intercept_ones,X]\n",
    "    return X_b\n",
    "\n",
    "# from the standardization article\n",
    "def standardize(X_train, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train_s = (X_train - mean) / std \n",
    "    X_test_s = (X_test - mean) / std \n",
    "    return X_train_s, X_test_s, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1.25, 1.  , 0.75, 1.5, 1.75, 1.5 , 0.75])\n",
    "y = np.array([40. , 42. , 46. , 37., 40. , 38. , 39.8])\n",
    "\n",
    "X_train, X_test = X[:4], X[4:]\n",
    "y_train, y_test = y[:4], y[4:]\n",
    "\n",
    "X_train_s, X_test_s, _, _ = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, label=\"train\")\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "plt.xlabel(\"age (years)\")\n",
    "plt.ylabel(\"price (€)\")\n",
    "plt.title(\"Figure Prices\")\n",
    "plt.grid(color = 'white', alpha=0.5, linestyle = '--', linewidth = 0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgradients\n",
    "\n",
    "$$\\text{A vector } g \\in \\mathbb{R}^d \\text{ is a subgradient of a function } \\\\\n",
    "f: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\text{ at the point } x_0 \\text{ if for all points } x \\text{ the following holds:} \\\\\n",
    "f(x) \\geq  {\\color{#26a6ed}f(x_0)} + g(x {\\color{#26a6ed} - x_0})\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elastic_mse_function(a2=1, a1=0):\n",
    "    \n",
    "    def elastic_mse(y, y_predicted, theta):\n",
    "        error = y-y_predicted\n",
    "        loss = 1/(y.size) * np.dot(error.T, error) + a2 * np.dot(theta,theta) + a1 * np.sum(np.abs(theta))\n",
    "        return loss\n",
    "    \n",
    "    return elastic_mse\n",
    "\n",
    "\n",
    "def get_elastic_gradient_function(a2=1, a1=0):\n",
    "    \n",
    "    def elastic_gradient(X_b, y, y_pred, theta):\n",
    "        return  -(2/y.size) * X_b.T.dot((y - y_pred)) + a1 * np.sign(theta) + 2 * a2 * theta\n",
    "\n",
    "    return elastic_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(X, y, theta, criterion, subgradient_function, number_of_iterations, learning_rate):\n",
    "    X_b = add_intercept_ones(X)\n",
    "    for i in range(number_of_iterations):\n",
    "\n",
    "        # predict and calculate loss\n",
    "        f = create_function(theta) # create the current function\n",
    "        y_predicted = f(X_b) # predict our entire x\n",
    "        loss = criterion(y, y_predicted, theta) # calculate the error\n",
    "\n",
    "        # perform optimization\n",
    "        subgradient = subgradient_function(X_b, y, y_predicted, theta) # calculate gradient\n",
    "        theta = theta - learning_rate * subgradient #adjust m and b\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.75       -2.74229857]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.rand(2)\n",
    "number_of_iterations = 3000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# important: use standardized data!\n",
    "theta_lasso_subgd = subgradient_descent(X_train_s, y_train, theta,\n",
    "                    get_elastic_mse_function(a2=0, a1=1),\n",
    "                    get_elastic_gradient_function(a2=0, a1=1),\n",
    "                    number_of_iterations, learning_rate)\n",
    "print(theta_lasso_subgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7636056f1e4d8fdf67bea993a39219552fd16254f6bf1d4cfd5ce8af047e9af1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
